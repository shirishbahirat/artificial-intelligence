{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pong.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirishbahirat/artificial-intelligence/blob/master/pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "w0mhtwKkdgg9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "import scipy.ndimage\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8BXwy3cTdmy6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Gamespace Variables\n",
        "\n",
        "env = gym.make(\"Pong-v0\") # environment info\n",
        "xs,rs,ys = [],[],[] # State, Reward, Action History\n",
        "average_reward = None # Reward for measuring average performance\n",
        "G = 0\n",
        "n_episode = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NXayixLQdoQ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_input = 80**2                # size of input\n",
        "n_hidden = 200                 # number of hidden layer neurons\n",
        "n_actions = 3                  # number of available actions i.e. fire (do nothing), left & right\n",
        "learning_rate = 1e-3\n",
        "gamma = .99                    # discount factor for reward\n",
        "decay1, decay2 = 0.9, 0.999    # decay rates for Adam optimizer\n",
        "save_path='.\\model\\'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iA_uZVZxemOI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CKyiBNEuduB0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf_model = {}\n",
        "# Initialize random weights using xavier initialization\n",
        "with tf.variable_scope('weights_one',reuse=False):\n",
        "    xavier_l1 = tf.truncated_normal_initializer(mean=0, stddev=1./np.sqrt(n_input), dtype=tf.float32)\n",
        "    tf_model['W1'] = tf.get_variable(\"W1\", [n_input, n_hidden], initializer=xavier_l1)\n",
        "with tf.variable_scope('weights_two',reuse=False):\n",
        "    xavier_l2 = tf.truncated_normal_initializer(mean=0, stddev=1./np.sqrt(n_hidden), dtype=tf.float32)\n",
        "    tf_model['W2'] = tf.get_variable(\"W2\", [n_hidden,n_actions], initializer=xavier_l2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "63tCCmdoeAvS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Functions\n",
        "\n",
        "# Crop, greyscale, and reshape pixels, returned flattened\n",
        "def resize(img):\n",
        "    i = img[35:195,:] # Crop out uneccessary top and bottom sections\n",
        "    i = np.around(np.mean(i,-1)) # Get grayscale & round to nearest integar\n",
        "    i[i==78] = 0 # Get rid of background\n",
        "    i[159,:] = 0 # Get rid of bottom line\n",
        "    i = scipy.misc.imresize(i, [80,80],interp='nearest') # Resize\n",
        "    i[i != 0] = 1 # Set everything else to 1 i.e. paddles and ball\n",
        "    return i.astype(np.float).ravel() # Returns pixels in flattened state\n",
        "           \n",
        "# Apply reward discounting to a series of rewards\n",
        "def tf_discount(tf_r): #tf_r ~ [game_steps,1]\n",
        "    discount_f = lambda a, v: a*gamma + v;  # Function for calculating discounted reward\n",
        "    tf_r_reverse = tf.scan(discount_f, tf.reverse(tf_r,[True, False])) # Reverses and applies discounts\n",
        "    tf_discounted_r = tf.reverse(tf_r_reverse,[True, False]) # Reverse back so in ascending time steps\n",
        "    return tf_discounted_r\n",
        "\n",
        "# Feed input and calculate action using weights\n",
        "def tf_policy_forward(x): #x ~ [1,D]\n",
        "    h = tf.nn.relu(tf.matmul(x, tf_model['W1'])) # Calulates hidden layer\n",
        "    logp = tf.matmul(h, tf_model['W2']) # Calculates output value\n",
        "    p = tf.nn.softmax(logp) # Converts out values into a probability i.e. sum of outputs = 1\n",
        "    return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jLN_rpE5eGpe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TF Placeholders\n",
        "X = tf.placeholder(dtype=tf.float32, shape=[None, n_input],name=\"X\")\n",
        "Y = tf.placeholder(dtype=tf.float32, shape=[None, n_actions],name=\"Y\")\n",
        "tf_epr = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"tf_epr\")\n",
        "\n",
        "# Discount rewards \n",
        "tf_discounted_g = tf_discount(tf_epr)\n",
        "# Normalize rewards\n",
        "tf_mean, tf_variance = tf.nn.moments(tf_discounted_g, [0], shift=None, name=\"reward_moments\")\n",
        "tf_discounted_g -= tf_mean\n",
        "tf_discounted_g /= tf.sqrt(tf_variance + 1e-6)\n",
        "\n",
        "# TF optimizer op\n",
        "tf_aprob = tf_policy_forward(X)\n",
        "loss = tf.nn.l2_loss(Y - tf_aprob)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate, beta1=decay1, beta2=decay2)\n",
        "#optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=decay1)\n",
        "tf_grads = optimizer.compute_gradients(loss, var_list=tf.trainable_variables(), grad_loss=tf_discounted_g)\n",
        "train_op = optimizer.apply_gradients(tf_grads)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "98ypKt5reLwr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TF Graph initialization\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZhfKufjePQU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load model if exists\n",
        "saver = tf.train.Saver(tf.global_variables())\n",
        "load_was_success = True \n",
        "try:\n",
        "    save_dir = '/'.join(save_path.split('/')[:-1])\n",
        "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
        "    load_path = ckpt.model_checkpoint_path\n",
        "    saver.restore(sess, load_path)\n",
        "except:\n",
        "    print(\"no saved model to load. starting new session\")\n",
        "    load_was_success = False\n",
        "else:\n",
        "    print(\"loaded model: {}\".format(load_path))\n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "    episode_number = int(load_path.split('-')[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y-zgjVCIen2Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2363
        },
        "outputId": "0925b4ba-cef6-48f9-a684-4ac4932d470a"
      },
      "cell_type": "code",
      "source": [
        "state1 = np.zeros([1,n_input])\n",
        "state2 = resize(env.reset())\n",
        "render = False\n",
        "\n",
        "while True:\n",
        "    # Decay learning rate\n",
        "    if n_episode > 4000:\n",
        "        learning_rate = 1e-4\n",
        "    elif n_episode > 5000:\n",
        "        learning_rate = 1e-5\n",
        "    elif n_episode > 6000:\n",
        "        learning_rate = 1e-6\n",
        "\n",
        "    # Subtract current state from previous state to visualize motion\n",
        "    state = state2 - state1\n",
        "    state1 = state2\n",
        "    \n",
        "    # Sample a action based from the network weights\n",
        "    feed = {X: np.reshape(state, (1,n_input))}\n",
        "    # Feed state forward through the network, returns an action between 0 and 2\n",
        "    aprob = sess.run(tf_aprob,feed) ; aprob = aprob[0,:]\n",
        "    # Choose random action based upon action probabilities\n",
        "    action = np.random.choice(n_actions, p=aprob)\n",
        "    # Convert action into one hot vector\n",
        "    label = np.zeros_like(aprob) ; label[action] = 1\n",
        "\n",
        "    # Take an action and get new variables, we add 1 to action based upon env.env.get_action_meanings()\n",
        "    state2, reward, done, info = env.step(action+1)\n",
        "    state2 = resize(state2)\n",
        "    if render:\n",
        "        env.render()\n",
        "    G += reward\n",
        "    \n",
        "    # Record state, action & reward history\n",
        "    xs.append(state) ; ys.append(label) ; rs.append(reward)\n",
        "    \n",
        "    if done:\n",
        "        # parameter update\n",
        "        feed = {X: np.vstack(xs), tf_epr: np.vstack(rs), Y: np.vstack(ys)}\n",
        "        _ = sess.run(train_op,feed)\n",
        "            \n",
        "        \n",
        "        # update average reward\n",
        "        average_reward = G if average_reward is None else average_reward * 0.99 + G * 0.01\n",
        "            \n",
        "        # print progress console\n",
        "        if n_episode % 10 == 0:\n",
        "            print('Episode {}: Reward: {}    Average Reward: {:4.3f}'.format(n_episode, G, average_reward))\n",
        "        else:\n",
        "            print('Episode {}: Reward: {}'.format(n_episode, G))\n",
        "        \n",
        "        # bookkeeping\n",
        "        \n",
        "        G = 0 # Reset episode reward\n",
        "        n_episode += 1 # the Next Episode\n",
        "        state1 = np.zeros([1,n_input])\n",
        "        state2 = resize(env.reset()) # Reset environment\n",
        "        xs,rs,ys = [],[],[] # reset game history\n",
        "        \n",
        "        if n_episode % 50 == 0:\n",
        "            saver.save(sess, save_path, global_step=n_episode)\n",
        "            print(\"SAVED MODEL #{}\".format(n_episode))\n",
        "        \n",
        "        if n_episode == 6000:\n",
        "            break\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode 0: Reward: -21.0    Average Reward: -21.000\n",
            "Episode 1: Reward: -19.0\n",
            "Episode 2: Reward: -18.0\n",
            "Episode 3: Reward: -21.0\n",
            "Episode 4: Reward: -20.0\n",
            "Episode 5: Reward: -20.0\n",
            "Episode 6: Reward: -20.0\n",
            "Episode 7: Reward: -21.0\n",
            "Episode 8: Reward: -20.0\n",
            "Episode 9: Reward: -18.0\n",
            "Episode 10: Reward: -21.0    Average Reward: -20.886\n",
            "Episode 11: Reward: -20.0\n",
            "Episode 12: Reward: -21.0\n",
            "Episode 13: Reward: -20.0\n",
            "Episode 14: Reward: -20.0\n",
            "Episode 15: Reward: -20.0\n",
            "Episode 16: Reward: -20.0\n",
            "Episode 17: Reward: -20.0\n",
            "Episode 18: Reward: -20.0\n",
            "Episode 19: Reward: -21.0\n",
            "Episode 20: Reward: -21.0    Average Reward: -20.830\n",
            "Episode 21: Reward: -21.0\n",
            "Episode 22: Reward: -21.0\n",
            "Episode 23: Reward: -21.0\n",
            "Episode 24: Reward: -19.0\n",
            "Episode 25: Reward: -21.0\n",
            "Episode 26: Reward: -21.0\n",
            "Episode 27: Reward: -19.0\n",
            "Episode 28: Reward: -21.0\n",
            "Episode 29: Reward: -20.0\n",
            "Episode 30: Reward: -21.0    Average Reward: -20.799\n",
            "Episode 31: Reward: -21.0\n",
            "Episode 32: Reward: -21.0\n",
            "Episode 33: Reward: -21.0\n",
            "Episode 34: Reward: -21.0\n",
            "Episode 35: Reward: -20.0\n",
            "Episode 36: Reward: -21.0\n",
            "Episode 37: Reward: -21.0\n",
            "Episode 38: Reward: -21.0\n",
            "Episode 39: Reward: -21.0\n",
            "Episode 40: Reward: -21.0    Average Reward: -20.808\n",
            "Episode 41: Reward: -19.0\n",
            "Episode 42: Reward: -21.0\n",
            "Episode 43: Reward: -21.0\n",
            "Episode 44: Reward: -21.0\n",
            "Episode 45: Reward: -21.0\n",
            "Episode 46: Reward: -21.0\n",
            "Episode 47: Reward: -21.0\n",
            "Episode 48: Reward: -19.0\n",
            "Episode 49: Reward: -19.0\n",
            "SAVED MODEL #50\n",
            "Episode 50: Reward: -20.0    Average Reward: -20.759\n",
            "Episode 51: Reward: -21.0\n",
            "Episode 52: Reward: -20.0\n",
            "Episode 53: Reward: -21.0\n",
            "Episode 54: Reward: -20.0\n",
            "Episode 55: Reward: -21.0\n",
            "Episode 56: Reward: -20.0\n",
            "Episode 57: Reward: -20.0\n",
            "Episode 58: Reward: -20.0\n",
            "Episode 59: Reward: -20.0\n",
            "Episode 60: Reward: -19.0    Average Reward: -20.704\n",
            "Episode 61: Reward: -20.0\n",
            "Episode 62: Reward: -21.0\n",
            "Episode 63: Reward: -20.0\n",
            "Episode 64: Reward: -21.0\n",
            "Episode 65: Reward: -19.0\n",
            "Episode 66: Reward: -20.0\n",
            "Episode 67: Reward: -21.0\n",
            "Episode 68: Reward: -21.0\n",
            "Episode 69: Reward: -19.0\n",
            "Episode 70: Reward: -21.0    Average Reward: -20.666\n",
            "Episode 71: Reward: -19.0\n",
            "Episode 72: Reward: -21.0\n",
            "Episode 73: Reward: -21.0\n",
            "Episode 74: Reward: -20.0\n",
            "Episode 75: Reward: -21.0\n",
            "Episode 76: Reward: -20.0\n",
            "Episode 77: Reward: -21.0\n",
            "Episode 78: Reward: -20.0\n",
            "Episode 79: Reward: -21.0\n",
            "Episode 80: Reward: -20.0    Average Reward: -20.641\n",
            "Episode 81: Reward: -21.0\n",
            "Episode 82: Reward: -21.0\n",
            "Episode 83: Reward: -21.0\n",
            "Episode 84: Reward: -21.0\n",
            "Episode 85: Reward: -20.0\n",
            "Episode 86: Reward: -20.0\n",
            "Episode 87: Reward: -20.0\n",
            "Episode 88: Reward: -21.0\n",
            "Episode 89: Reward: -21.0\n",
            "Episode 90: Reward: -21.0    Average Reward: -20.646\n",
            "Episode 91: Reward: -20.0\n",
            "Episode 92: Reward: -19.0\n",
            "Episode 93: Reward: -21.0\n",
            "Episode 94: Reward: -21.0\n",
            "Episode 95: Reward: -20.0\n",
            "Episode 96: Reward: -20.0\n",
            "Episode 97: Reward: -20.0\n",
            "Episode 98: Reward: -20.0\n",
            "Episode 99: Reward: -20.0\n",
            "SAVED MODEL #100\n",
            "Episode 100: Reward: -21.0    Average Reward: -20.604\n",
            "Episode 101: Reward: -21.0\n",
            "Episode 102: Reward: -20.0\n",
            "Episode 103: Reward: -20.0\n",
            "Episode 104: Reward: -20.0\n",
            "Episode 105: Reward: -20.0\n",
            "Episode 106: Reward: -21.0\n",
            "Episode 107: Reward: -20.0\n",
            "Episode 108: Reward: -20.0\n",
            "Episode 109: Reward: -20.0\n",
            "Episode 110: Reward: -20.0    Average Reward: -20.565\n",
            "Episode 111: Reward: -20.0\n",
            "Episode 112: Reward: -20.0\n",
            "Episode 113: Reward: -21.0\n",
            "Episode 114: Reward: -20.0\n",
            "Episode 115: Reward: -21.0\n",
            "Episode 116: Reward: -19.0\n",
            "Episode 117: Reward: -21.0\n",
            "Episode 118: Reward: -21.0\n",
            "Episode 119: Reward: -20.0\n",
            "Episode 120: Reward: -21.0    Average Reward: -20.550\n",
            "Episode 121: Reward: -20.0\n",
            "Episode 122: Reward: -21.0\n",
            "Episode 123: Reward: -19.0\n",
            "Episode 124: Reward: -20.0\n",
            "Episode 125: Reward: -20.0\n",
            "Episode 126: Reward: -19.0\n",
            "Episode 127: Reward: -21.0\n",
            "Episode 128: Reward: -20.0\n",
            "Episode 129: Reward: -21.0\n",
            "Episode 130: Reward: -19.0    Average Reward: -20.497\n",
            "Episode 131: Reward: -21.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iYjWVD6mez4p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "state1 = resize(env.reset())\n",
        "\n",
        "for i in range(50):\n",
        "    state2, _, _, _ = env.step(env.action_space.sample())\n",
        "    state2 = resize(state2)\n",
        "    if i > 15:\n",
        "        plt.imshow((state2-state1).reshape(80,80), cmap='gray')\n",
        "        plt.show()\n",
        "    state1 = state2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zRnaiGhDe1KC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "state1 = np.zeros([1,n_input])\n",
        "state2 = resize(env.reset())\n",
        "\n",
        "while True:\n",
        "    state = state2-state1\n",
        "    state1 = state2\n",
        "    env.render()\n",
        "    feed = {X: np.reshape(state, (1,n_input))}\n",
        "    aprob = sess.run(tf_aprob,feed) ; aprob = aprob[0,:]\n",
        "    action = np.argmax(aprob)+1\n",
        "    state2, reward, done, info = env.step(action)\n",
        "    state2 = resize(state2)\n",
        "    \n",
        "    if reward == 1:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CltFnR8He5st",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weightVal = sess.run(tf_model['W1'])\n",
        "\n",
        "for i in range(200):\n",
        "    plt.imshow(weightVal[:,i].reshape(80,80), cmap='coolwarm')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}