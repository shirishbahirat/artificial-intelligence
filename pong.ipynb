{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pong.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirishbahirat/artificial-intelligence/blob/master/pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "w0mhtwKkdgg9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "import scipy.ndimage\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8BXwy3cTdmy6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Gamespace Variables\n",
        "\n",
        "env = gym.make(\"Pong-v0\") # environment info\n",
        "xs,rs,ys = [],[],[] # State, Reward, Action History\n",
        "average_reward = None # Reward for measuring average performance\n",
        "G = 0\n",
        "n_episode = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NXayixLQdoQ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_input = 80**2                # size of input\n",
        "n_hidden = 200                 # number of hidden layer neurons\n",
        "n_actions = 3                  # number of available actions i.e. fire (do nothing), left & right\n",
        "learning_rate = 1e-3\n",
        "gamma = .99                    # discount factor for reward\n",
        "decay1, decay2 = 0.9, 0.999    # decay rates for Adam optimizer\n",
        "save_path='.\\model\\'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iA_uZVZxemOI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CKyiBNEuduB0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf_model = {}\n",
        "# Initialize random weights using xavier initialization\n",
        "with tf.variable_scope('weights_one',reuse=False):\n",
        "    xavier_l1 = tf.truncated_normal_initializer(mean=0, stddev=1./np.sqrt(n_input), dtype=tf.float32)\n",
        "    tf_model['W1'] = tf.get_variable(\"W1\", [n_input, n_hidden], initializer=xavier_l1)\n",
        "with tf.variable_scope('weights_two',reuse=False):\n",
        "    xavier_l2 = tf.truncated_normal_initializer(mean=0, stddev=1./np.sqrt(n_hidden), dtype=tf.float32)\n",
        "    tf_model['W2'] = tf.get_variable(\"W2\", [n_hidden,n_actions], initializer=xavier_l2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "63tCCmdoeAvS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Functions\n",
        "\n",
        "# Crop, greyscale, and reshape pixels, returned flattened\n",
        "def resize(img):\n",
        "    i = img[35:195,:] # Crop out uneccessary top and bottom sections\n",
        "    i = np.around(np.mean(i,-1)) # Get grayscale & round to nearest integar\n",
        "    i[i==78] = 0 # Get rid of background\n",
        "    i[159,:] = 0 # Get rid of bottom line\n",
        "    i = scipy.misc.imresize(i, [80,80],interp='nearest') # Resize\n",
        "    i[i != 0] = 1 # Set everything else to 1 i.e. paddles and ball\n",
        "    return i.astype(np.float).ravel() # Returns pixels in flattened state\n",
        "           \n",
        "# Apply reward discounting to a series of rewards\n",
        "def tf_discount(tf_r): #tf_r ~ [game_steps,1]\n",
        "    discount_f = lambda a, v: a*gamma + v;  # Function for calculating discounted reward\n",
        "    tf_r_reverse = tf.scan(discount_f, tf.reverse(tf_r,[True, False])) # Reverses and applies discounts\n",
        "    tf_discounted_r = tf.reverse(tf_r_reverse,[True, False]) # Reverse back so in ascending time steps\n",
        "    return tf_discounted_r\n",
        "\n",
        "# Feed input and calculate action using weights\n",
        "def tf_policy_forward(x): #x ~ [1,D]\n",
        "    h = tf.nn.relu(tf.matmul(x, tf_model['W1'])) # Calulates hidden layer\n",
        "    logp = tf.matmul(h, tf_model['W2']) # Calculates output value\n",
        "    p = tf.nn.softmax(logp) # Converts out values into a probability i.e. sum of outputs = 1\n",
        "    return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jLN_rpE5eGpe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TF Placeholders\n",
        "X = tf.placeholder(dtype=tf.float32, shape=[None, n_input],name=\"X\")\n",
        "Y = tf.placeholder(dtype=tf.float32, shape=[None, n_actions],name=\"Y\")\n",
        "tf_epr = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"tf_epr\")\n",
        "\n",
        "# Discount rewards \n",
        "tf_discounted_g = tf_discount(tf_epr)\n",
        "# Normalize rewards\n",
        "tf_mean, tf_variance = tf.nn.moments(tf_discounted_g, [0], shift=None, name=\"reward_moments\")\n",
        "tf_discounted_g -= tf_mean\n",
        "tf_discounted_g /= tf.sqrt(tf_variance + 1e-6)\n",
        "\n",
        "# TF optimizer op\n",
        "tf_aprob = tf_policy_forward(X)\n",
        "loss = tf.nn.l2_loss(Y - tf_aprob)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate, beta1=decay1, beta2=decay2)\n",
        "#optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=decay1)\n",
        "tf_grads = optimizer.compute_gradients(loss, var_list=tf.trainable_variables(), grad_loss=tf_discounted_g)\n",
        "train_op = optimizer.apply_gradients(tf_grads)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "98ypKt5reLwr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TF Graph initialization\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZhfKufjePQU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load model if exists\n",
        "saver = tf.train.Saver(tf.global_variables())\n",
        "load_was_success = True \n",
        "try:\n",
        "    save_dir = '/'.join(save_path.split('/')[:-1])\n",
        "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
        "    load_path = ckpt.model_checkpoint_path\n",
        "    saver.restore(sess, load_path)\n",
        "except:\n",
        "    print(\"no saved model to load. starting new session\")\n",
        "    load_was_success = False\n",
        "else:\n",
        "    print(\"loaded model: {}\".format(load_path))\n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "    episode_number = int(load_path.split('-')[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y-zgjVCIen2Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13212
        },
        "outputId": "c4d33700-eee4-43ee-ab72-7f31c8481e5e"
      },
      "cell_type": "code",
      "source": [
        "state1 = np.zeros([1,n_input])\n",
        "state2 = resize(env.reset())\n",
        "render = False\n",
        "\n",
        "while True:\n",
        "    # Decay learning rate\n",
        "    if n_episode > 4000:\n",
        "        learning_rate = 1e-4\n",
        "    elif n_episode > 5000:\n",
        "        learning_rate = 1e-5\n",
        "    elif n_episode > 6000:\n",
        "        learning_rate = 1e-6\n",
        "\n",
        "    # Subtract current state from previous state to visualize motion\n",
        "    state = state2 - state1\n",
        "    state1 = state2\n",
        "    \n",
        "    # Sample a action based from the network weights\n",
        "    feed = {X: np.reshape(state, (1,n_input))}\n",
        "    # Feed state forward through the network, returns an action between 0 and 2\n",
        "    aprob = sess.run(tf_aprob,feed) ; aprob = aprob[0,:]\n",
        "    # Choose random action based upon action probabilities\n",
        "    action = np.random.choice(n_actions, p=aprob)\n",
        "    # Convert action into one hot vector\n",
        "    label = np.zeros_like(aprob) ; label[action] = 1\n",
        "\n",
        "    # Take an action and get new variables, we add 1 to action based upon env.env.get_action_meanings()\n",
        "    state2, reward, done, info = env.step(action+1)\n",
        "    state2 = resize(state2)\n",
        "    if render:\n",
        "        env.render()\n",
        "    G += reward\n",
        "    \n",
        "    # Record state, action & reward history\n",
        "    xs.append(state) ; ys.append(label) ; rs.append(reward)\n",
        "    \n",
        "    if done:\n",
        "        # parameter update\n",
        "        feed = {X: np.vstack(xs), tf_epr: np.vstack(rs), Y: np.vstack(ys)}\n",
        "        _ = sess.run(train_op,feed)\n",
        "            \n",
        "        \n",
        "        # update average reward\n",
        "        average_reward = G if average_reward is None else average_reward * 0.99 + G * 0.01\n",
        "            \n",
        "        # print progress console\n",
        "        if n_episode % 10 == 0:\n",
        "            print('Episode {}: Reward: {}    Average Reward: {:4.3f}'.format(n_episode, G, average_reward))\n",
        "        else:\n",
        "            print('Episode {}: Reward: {}'.format(n_episode, G))\n",
        "        \n",
        "        # bookkeeping\n",
        "        \n",
        "        G = 0 # Reset episode reward\n",
        "        n_episode += 1 # the Next Episode\n",
        "        state1 = np.zeros([1,n_input])\n",
        "        state2 = resize(env.reset()) # Reset environment\n",
        "        xs,rs,ys = [],[],[] # reset game history\n",
        "        \n",
        "        if n_episode % 50 == 0:\n",
        "            saver.save(sess, save_path, global_step=n_episode)\n",
        "            print(\"SAVED MODEL #{}\".format(n_episode))\n",
        "        \n",
        "        if n_episode == 6000:\n",
        "            break\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode 0: Reward: -21.0    Average Reward: -21.000\n",
            "Episode 1: Reward: -19.0\n",
            "Episode 2: Reward: -18.0\n",
            "Episode 3: Reward: -21.0\n",
            "Episode 4: Reward: -20.0\n",
            "Episode 5: Reward: -20.0\n",
            "Episode 6: Reward: -20.0\n",
            "Episode 7: Reward: -21.0\n",
            "Episode 8: Reward: -20.0\n",
            "Episode 9: Reward: -18.0\n",
            "Episode 10: Reward: -21.0    Average Reward: -20.886\n",
            "Episode 11: Reward: -20.0\n",
            "Episode 12: Reward: -21.0\n",
            "Episode 13: Reward: -20.0\n",
            "Episode 14: Reward: -20.0\n",
            "Episode 15: Reward: -20.0\n",
            "Episode 16: Reward: -20.0\n",
            "Episode 17: Reward: -20.0\n",
            "Episode 18: Reward: -20.0\n",
            "Episode 19: Reward: -21.0\n",
            "Episode 20: Reward: -21.0    Average Reward: -20.830\n",
            "Episode 21: Reward: -21.0\n",
            "Episode 22: Reward: -21.0\n",
            "Episode 23: Reward: -21.0\n",
            "Episode 24: Reward: -19.0\n",
            "Episode 25: Reward: -21.0\n",
            "Episode 26: Reward: -21.0\n",
            "Episode 27: Reward: -19.0\n",
            "Episode 28: Reward: -21.0\n",
            "Episode 29: Reward: -20.0\n",
            "Episode 30: Reward: -21.0    Average Reward: -20.799\n",
            "Episode 31: Reward: -21.0\n",
            "Episode 32: Reward: -21.0\n",
            "Episode 33: Reward: -21.0\n",
            "Episode 34: Reward: -21.0\n",
            "Episode 35: Reward: -20.0\n",
            "Episode 36: Reward: -21.0\n",
            "Episode 37: Reward: -21.0\n",
            "Episode 38: Reward: -21.0\n",
            "Episode 39: Reward: -21.0\n",
            "Episode 40: Reward: -21.0    Average Reward: -20.808\n",
            "Episode 41: Reward: -19.0\n",
            "Episode 42: Reward: -21.0\n",
            "Episode 43: Reward: -21.0\n",
            "Episode 44: Reward: -21.0\n",
            "Episode 45: Reward: -21.0\n",
            "Episode 46: Reward: -21.0\n",
            "Episode 47: Reward: -21.0\n",
            "Episode 48: Reward: -19.0\n",
            "Episode 49: Reward: -19.0\n",
            "SAVED MODEL #50\n",
            "Episode 50: Reward: -20.0    Average Reward: -20.759\n",
            "Episode 51: Reward: -21.0\n",
            "Episode 52: Reward: -20.0\n",
            "Episode 53: Reward: -21.0\n",
            "Episode 54: Reward: -20.0\n",
            "Episode 55: Reward: -21.0\n",
            "Episode 56: Reward: -20.0\n",
            "Episode 57: Reward: -20.0\n",
            "Episode 58: Reward: -20.0\n",
            "Episode 59: Reward: -20.0\n",
            "Episode 60: Reward: -19.0    Average Reward: -20.704\n",
            "Episode 61: Reward: -20.0\n",
            "Episode 62: Reward: -21.0\n",
            "Episode 63: Reward: -20.0\n",
            "Episode 64: Reward: -21.0\n",
            "Episode 65: Reward: -19.0\n",
            "Episode 66: Reward: -20.0\n",
            "Episode 67: Reward: -21.0\n",
            "Episode 68: Reward: -21.0\n",
            "Episode 69: Reward: -19.0\n",
            "Episode 70: Reward: -21.0    Average Reward: -20.666\n",
            "Episode 71: Reward: -19.0\n",
            "Episode 72: Reward: -21.0\n",
            "Episode 73: Reward: -21.0\n",
            "Episode 74: Reward: -20.0\n",
            "Episode 75: Reward: -21.0\n",
            "Episode 76: Reward: -20.0\n",
            "Episode 77: Reward: -21.0\n",
            "Episode 78: Reward: -20.0\n",
            "Episode 79: Reward: -21.0\n",
            "Episode 80: Reward: -20.0    Average Reward: -20.641\n",
            "Episode 81: Reward: -21.0\n",
            "Episode 82: Reward: -21.0\n",
            "Episode 83: Reward: -21.0\n",
            "Episode 84: Reward: -21.0\n",
            "Episode 85: Reward: -20.0\n",
            "Episode 86: Reward: -20.0\n",
            "Episode 87: Reward: -20.0\n",
            "Episode 88: Reward: -21.0\n",
            "Episode 89: Reward: -21.0\n",
            "Episode 90: Reward: -21.0    Average Reward: -20.646\n",
            "Episode 91: Reward: -20.0\n",
            "Episode 92: Reward: -19.0\n",
            "Episode 93: Reward: -21.0\n",
            "Episode 94: Reward: -21.0\n",
            "Episode 95: Reward: -20.0\n",
            "Episode 96: Reward: -20.0\n",
            "Episode 97: Reward: -20.0\n",
            "Episode 98: Reward: -20.0\n",
            "Episode 99: Reward: -20.0\n",
            "SAVED MODEL #100\n",
            "Episode 100: Reward: -21.0    Average Reward: -20.604\n",
            "Episode 101: Reward: -21.0\n",
            "Episode 102: Reward: -20.0\n",
            "Episode 103: Reward: -20.0\n",
            "Episode 104: Reward: -20.0\n",
            "Episode 105: Reward: -20.0\n",
            "Episode 106: Reward: -21.0\n",
            "Episode 107: Reward: -20.0\n",
            "Episode 108: Reward: -20.0\n",
            "Episode 109: Reward: -20.0\n",
            "Episode 110: Reward: -20.0    Average Reward: -20.565\n",
            "Episode 111: Reward: -20.0\n",
            "Episode 112: Reward: -20.0\n",
            "Episode 113: Reward: -21.0\n",
            "Episode 114: Reward: -20.0\n",
            "Episode 115: Reward: -21.0\n",
            "Episode 116: Reward: -19.0\n",
            "Episode 117: Reward: -21.0\n",
            "Episode 118: Reward: -21.0\n",
            "Episode 119: Reward: -20.0\n",
            "Episode 120: Reward: -21.0    Average Reward: -20.550\n",
            "Episode 121: Reward: -20.0\n",
            "Episode 122: Reward: -21.0\n",
            "Episode 123: Reward: -19.0\n",
            "Episode 124: Reward: -20.0\n",
            "Episode 125: Reward: -20.0\n",
            "Episode 126: Reward: -19.0\n",
            "Episode 127: Reward: -21.0\n",
            "Episode 128: Reward: -20.0\n",
            "Episode 129: Reward: -21.0\n",
            "Episode 130: Reward: -19.0    Average Reward: -20.497\n",
            "Episode 131: Reward: -21.0\n",
            "Episode 132: Reward: -21.0\n",
            "Episode 133: Reward: -21.0\n",
            "Episode 134: Reward: -21.0\n",
            "Episode 135: Reward: -20.0\n",
            "Episode 136: Reward: -21.0\n",
            "Episode 137: Reward: -21.0\n",
            "Episode 138: Reward: -20.0\n",
            "Episode 139: Reward: -20.0\n",
            "Episode 140: Reward: -18.0    Average Reward: -20.486\n",
            "Episode 141: Reward: -21.0\n",
            "Episode 142: Reward: -20.0\n",
            "Episode 143: Reward: -20.0\n",
            "Episode 144: Reward: -19.0\n",
            "Episode 145: Reward: -20.0\n",
            "Episode 146: Reward: -20.0\n",
            "Episode 147: Reward: -21.0\n",
            "Episode 148: Reward: -20.0\n",
            "Episode 149: Reward: -20.0\n",
            "SAVED MODEL #150\n",
            "Episode 150: Reward: -20.0    Average Reward: -20.449\n",
            "Episode 151: Reward: -20.0\n",
            "Episode 152: Reward: -20.0\n",
            "Episode 153: Reward: -21.0\n",
            "Episode 154: Reward: -19.0\n",
            "Episode 155: Reward: -21.0\n",
            "Episode 156: Reward: -20.0\n",
            "Episode 157: Reward: -20.0\n",
            "Episode 158: Reward: -20.0\n",
            "Episode 159: Reward: -21.0\n",
            "Episode 160: Reward: -20.0    Average Reward: -20.425\n",
            "Episode 161: Reward: -19.0\n",
            "Episode 162: Reward: -20.0\n",
            "Episode 163: Reward: -20.0\n",
            "Episode 164: Reward: -20.0\n",
            "Episode 165: Reward: -20.0\n",
            "Episode 166: Reward: -18.0\n",
            "Episode 167: Reward: -20.0\n",
            "Episode 168: Reward: -21.0\n",
            "Episode 169: Reward: -19.0\n",
            "Episode 170: Reward: -19.0    Average Reward: -20.346\n",
            "Episode 171: Reward: -20.0\n",
            "Episode 172: Reward: -21.0\n",
            "Episode 173: Reward: -20.0\n",
            "Episode 174: Reward: -20.0\n",
            "Episode 175: Reward: -19.0\n",
            "Episode 176: Reward: -21.0\n",
            "Episode 177: Reward: -20.0\n",
            "Episode 178: Reward: -20.0\n",
            "Episode 179: Reward: -20.0\n",
            "Episode 180: Reward: -20.0    Average Reward: -20.322\n",
            "Episode 181: Reward: -20.0\n",
            "Episode 182: Reward: -20.0\n",
            "Episode 183: Reward: -21.0\n",
            "Episode 184: Reward: -20.0\n",
            "Episode 185: Reward: -20.0\n",
            "Episode 186: Reward: -19.0\n",
            "Episode 187: Reward: -20.0\n",
            "Episode 188: Reward: -20.0\n",
            "Episode 189: Reward: -20.0\n",
            "Episode 190: Reward: -20.0    Average Reward: -20.291\n",
            "Episode 191: Reward: -19.0\n",
            "Episode 192: Reward: -21.0\n",
            "Episode 193: Reward: -20.0\n",
            "Episode 194: Reward: -19.0\n",
            "Episode 195: Reward: -19.0\n",
            "Episode 196: Reward: -21.0\n",
            "Episode 197: Reward: -20.0\n",
            "Episode 198: Reward: -21.0\n",
            "Episode 199: Reward: -20.0\n",
            "SAVED MODEL #200\n",
            "Episode 200: Reward: -21.0    Average Reward: -20.274\n",
            "Episode 201: Reward: -21.0\n",
            "Episode 202: Reward: -19.0\n",
            "Episode 203: Reward: -21.0\n",
            "Episode 204: Reward: -21.0\n",
            "Episode 205: Reward: -20.0\n",
            "Episode 206: Reward: -21.0\n",
            "Episode 207: Reward: -21.0\n",
            "Episode 208: Reward: -21.0\n",
            "Episode 209: Reward: -21.0\n",
            "Episode 210: Reward: -21.0    Average Reward: -20.315\n",
            "Episode 211: Reward: -20.0\n",
            "Episode 212: Reward: -21.0\n",
            "Episode 213: Reward: -21.0\n",
            "Episode 214: Reward: -21.0\n",
            "Episode 215: Reward: -20.0\n",
            "Episode 216: Reward: -21.0\n",
            "Episode 217: Reward: -21.0\n",
            "Episode 218: Reward: -20.0\n",
            "Episode 219: Reward: -20.0\n",
            "Episode 220: Reward: -20.0    Average Reward: -20.333\n",
            "Episode 221: Reward: -21.0\n",
            "Episode 222: Reward: -20.0\n",
            "Episode 223: Reward: -19.0\n",
            "Episode 224: Reward: -20.0\n",
            "Episode 225: Reward: -19.0\n",
            "Episode 226: Reward: -21.0\n",
            "Episode 227: Reward: -21.0\n",
            "Episode 228: Reward: -19.0\n",
            "Episode 229: Reward: -20.0\n",
            "Episode 230: Reward: -20.0    Average Reward: -20.301\n",
            "Episode 231: Reward: -21.0\n",
            "Episode 232: Reward: -20.0\n",
            "Episode 233: Reward: -21.0\n",
            "Episode 234: Reward: -20.0\n",
            "Episode 235: Reward: -21.0\n",
            "Episode 236: Reward: -21.0\n",
            "Episode 237: Reward: -21.0\n",
            "Episode 238: Reward: -21.0\n",
            "Episode 239: Reward: -21.0\n",
            "Episode 240: Reward: -20.0    Average Reward: -20.339\n",
            "Episode 241: Reward: -21.0\n",
            "Episode 242: Reward: -18.0\n",
            "Episode 243: Reward: -20.0\n",
            "Episode 244: Reward: -20.0\n",
            "Episode 245: Reward: -20.0\n",
            "Episode 246: Reward: -20.0\n",
            "Episode 247: Reward: -19.0\n",
            "Episode 248: Reward: -21.0\n",
            "Episode 249: Reward: -21.0\n",
            "SAVED MODEL #250\n",
            "Episode 250: Reward: -19.0    Average Reward: -20.297\n",
            "Episode 251: Reward: -20.0\n",
            "Episode 252: Reward: -21.0\n",
            "Episode 253: Reward: -20.0\n",
            "Episode 254: Reward: -20.0\n",
            "Episode 255: Reward: -20.0\n",
            "Episode 256: Reward: -20.0\n",
            "Episode 257: Reward: -20.0\n",
            "Episode 258: Reward: -20.0\n",
            "Episode 259: Reward: -20.0\n",
            "Episode 260: Reward: -21.0    Average Reward: -20.288\n",
            "Episode 261: Reward: -19.0\n",
            "Episode 262: Reward: -21.0\n",
            "Episode 263: Reward: -21.0\n",
            "Episode 264: Reward: -21.0\n",
            "Episode 265: Reward: -21.0\n",
            "Episode 266: Reward: -21.0\n",
            "Episode 267: Reward: -20.0\n",
            "Episode 268: Reward: -20.0\n",
            "Episode 269: Reward: -20.0\n",
            "Episode 270: Reward: -18.0    Average Reward: -20.278\n",
            "Episode 271: Reward: -21.0\n",
            "Episode 272: Reward: -20.0\n",
            "Episode 273: Reward: -21.0\n",
            "Episode 274: Reward: -19.0\n",
            "Episode 275: Reward: -20.0\n",
            "Episode 276: Reward: -21.0\n",
            "Episode 277: Reward: -21.0\n",
            "Episode 278: Reward: -20.0\n",
            "Episode 279: Reward: -16.0\n",
            "Episode 280: Reward: -20.0    Average Reward: -20.240\n",
            "Episode 281: Reward: -20.0\n",
            "Episode 282: Reward: -20.0\n",
            "Episode 283: Reward: -19.0\n",
            "Episode 284: Reward: -18.0\n",
            "Episode 285: Reward: -21.0\n",
            "Episode 286: Reward: -20.0\n",
            "Episode 287: Reward: -18.0\n",
            "Episode 288: Reward: -19.0\n",
            "Episode 289: Reward: -20.0\n",
            "Episode 290: Reward: -18.0    Average Reward: -20.150\n",
            "Episode 291: Reward: -21.0\n",
            "Episode 292: Reward: -21.0\n",
            "Episode 293: Reward: -20.0\n",
            "Episode 294: Reward: -19.0\n",
            "Episode 295: Reward: -20.0\n",
            "Episode 296: Reward: -17.0\n",
            "Episode 297: Reward: -20.0\n",
            "Episode 298: Reward: -20.0\n",
            "Episode 299: Reward: -19.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "SAVED MODEL #300\n",
            "Episode 300: Reward: -19.0    Average Reward: -20.096\n",
            "Episode 301: Reward: -21.0\n",
            "Episode 302: Reward: -19.0\n",
            "Episode 303: Reward: -21.0\n",
            "Episode 304: Reward: -21.0\n",
            "Episode 305: Reward: -20.0\n",
            "Episode 306: Reward: -20.0\n",
            "Episode 307: Reward: -21.0\n",
            "Episode 308: Reward: -19.0\n",
            "Episode 309: Reward: -21.0\n",
            "Episode 310: Reward: -21.0    Average Reward: -20.125\n",
            "Episode 311: Reward: -20.0\n",
            "Episode 312: Reward: -19.0\n",
            "Episode 313: Reward: -20.0\n",
            "Episode 314: Reward: -19.0\n",
            "Episode 315: Reward: -21.0\n",
            "Episode 316: Reward: -19.0\n",
            "Episode 317: Reward: -21.0\n",
            "Episode 318: Reward: -21.0\n",
            "Episode 319: Reward: -19.0\n",
            "Episode 320: Reward: -18.0    Average Reward: -20.084\n",
            "Episode 321: Reward: -19.0\n",
            "Episode 322: Reward: -21.0\n",
            "Episode 323: Reward: -20.0\n",
            "Episode 324: Reward: -21.0\n",
            "Episode 325: Reward: -21.0\n",
            "Episode 326: Reward: -18.0\n",
            "Episode 327: Reward: -21.0\n",
            "Episode 328: Reward: -18.0\n",
            "Episode 329: Reward: -19.0\n",
            "Episode 330: Reward: -18.0    Average Reward: -20.036\n",
            "Episode 331: Reward: -15.0\n",
            "Episode 332: Reward: -20.0\n",
            "Episode 333: Reward: -21.0\n",
            "Episode 334: Reward: -20.0\n",
            "Episode 335: Reward: -18.0\n",
            "Episode 336: Reward: -20.0\n",
            "Episode 337: Reward: -21.0\n",
            "Episode 338: Reward: -21.0\n",
            "Episode 339: Reward: -20.0\n",
            "Episode 340: Reward: -18.0    Average Reward: -19.976\n",
            "Episode 341: Reward: -20.0\n",
            "Episode 342: Reward: -20.0\n",
            "Episode 343: Reward: -19.0\n",
            "Episode 344: Reward: -20.0\n",
            "Episode 345: Reward: -21.0\n",
            "Episode 346: Reward: -20.0\n",
            "Episode 347: Reward: -19.0\n",
            "Episode 348: Reward: -20.0\n",
            "Episode 349: Reward: -21.0\n",
            "SAVED MODEL #350\n",
            "Episode 350: Reward: -19.0    Average Reward: -19.969\n",
            "Episode 351: Reward: -21.0\n",
            "Episode 352: Reward: -20.0\n",
            "Episode 353: Reward: -20.0\n",
            "Episode 354: Reward: -20.0\n",
            "Episode 355: Reward: -21.0\n",
            "Episode 356: Reward: -19.0\n",
            "Episode 357: Reward: -21.0\n",
            "Episode 358: Reward: -20.0\n",
            "Episode 359: Reward: -18.0\n",
            "Episode 360: Reward: -21.0    Average Reward: -19.981\n",
            "Episode 361: Reward: -19.0\n",
            "Episode 362: Reward: -16.0\n",
            "Episode 363: Reward: -19.0\n",
            "Episode 364: Reward: -19.0\n",
            "Episode 365: Reward: -20.0\n",
            "Episode 366: Reward: -19.0\n",
            "Episode 367: Reward: -20.0\n",
            "Episode 368: Reward: -21.0\n",
            "Episode 369: Reward: -16.0\n",
            "Episode 370: Reward: -18.0    Average Reward: -19.859\n",
            "Episode 371: Reward: -20.0\n",
            "Episode 372: Reward: -18.0\n",
            "Episode 373: Reward: -19.0\n",
            "Episode 374: Reward: -21.0\n",
            "Episode 375: Reward: -20.0\n",
            "Episode 376: Reward: -20.0\n",
            "Episode 377: Reward: -20.0\n",
            "Episode 378: Reward: -21.0\n",
            "Episode 379: Reward: -20.0\n",
            "Episode 380: Reward: -21.0    Average Reward: -19.874\n",
            "Episode 381: Reward: -20.0\n",
            "Episode 382: Reward: -21.0\n",
            "Episode 383: Reward: -19.0\n",
            "Episode 384: Reward: -21.0\n",
            "Episode 385: Reward: -17.0\n",
            "Episode 386: Reward: -18.0\n",
            "Episode 387: Reward: -16.0\n",
            "Episode 388: Reward: -19.0\n",
            "Episode 389: Reward: -21.0\n",
            "Episode 390: Reward: -18.0    Average Reward: -19.789\n",
            "Episode 391: Reward: -20.0\n",
            "Episode 392: Reward: -18.0\n",
            "Episode 393: Reward: -18.0\n",
            "Episode 394: Reward: -18.0\n",
            "Episode 395: Reward: -19.0\n",
            "Episode 396: Reward: -20.0\n",
            "Episode 397: Reward: -20.0\n",
            "Episode 398: Reward: -20.0\n",
            "Episode 399: Reward: -19.0\n",
            "SAVED MODEL #400\n",
            "Episode 400: Reward: -19.0    Average Reward: -19.723\n",
            "Episode 401: Reward: -17.0\n",
            "Episode 402: Reward: -20.0\n",
            "Episode 403: Reward: -18.0\n",
            "Episode 404: Reward: -21.0\n",
            "Episode 405: Reward: -19.0\n",
            "Episode 406: Reward: -19.0\n",
            "Episode 407: Reward: -21.0\n",
            "Episode 408: Reward: -19.0\n",
            "Episode 409: Reward: -21.0\n",
            "Episode 410: Reward: -20.0    Average Reward: -19.704\n",
            "Episode 411: Reward: -19.0\n",
            "Episode 412: Reward: -19.0\n",
            "Episode 413: Reward: -18.0\n",
            "Episode 414: Reward: -19.0\n",
            "Episode 415: Reward: -16.0\n",
            "Episode 416: Reward: -20.0\n",
            "Episode 417: Reward: -20.0\n",
            "Episode 418: Reward: -20.0\n",
            "Episode 419: Reward: -19.0\n",
            "Episode 420: Reward: -19.0    Average Reward: -19.628\n",
            "Episode 421: Reward: -19.0\n",
            "Episode 422: Reward: -19.0\n",
            "Episode 423: Reward: -21.0\n",
            "Episode 424: Reward: -19.0\n",
            "Episode 425: Reward: -18.0\n",
            "Episode 426: Reward: -20.0\n",
            "Episode 427: Reward: -20.0\n",
            "Episode 428: Reward: -19.0\n",
            "Episode 429: Reward: -19.0\n",
            "Episode 430: Reward: -20.0    Average Reward: -19.606\n",
            "Episode 431: Reward: -20.0\n",
            "Episode 432: Reward: -19.0\n",
            "Episode 433: Reward: -18.0\n",
            "Episode 434: Reward: -20.0\n",
            "Episode 435: Reward: -19.0\n",
            "Episode 436: Reward: -19.0\n",
            "Episode 437: Reward: -18.0\n",
            "Episode 438: Reward: -17.0\n",
            "Episode 439: Reward: -17.0\n",
            "Episode 440: Reward: -20.0    Average Reward: -19.518\n",
            "Episode 441: Reward: -20.0\n",
            "Episode 442: Reward: -20.0\n",
            "Episode 443: Reward: -19.0\n",
            "Episode 444: Reward: -19.0\n",
            "Episode 445: Reward: -19.0\n",
            "Episode 446: Reward: -18.0\n",
            "Episode 447: Reward: -20.0\n",
            "Episode 448: Reward: -17.0\n",
            "Episode 449: Reward: -18.0\n",
            "SAVED MODEL #450\n",
            "Episode 450: Reward: -18.0    Average Reward: -19.448\n",
            "Episode 451: Reward: -16.0\n",
            "Episode 452: Reward: -18.0\n",
            "Episode 453: Reward: -18.0\n",
            "Episode 454: Reward: -20.0\n",
            "Episode 455: Reward: -19.0\n",
            "Episode 456: Reward: -21.0\n",
            "Episode 457: Reward: -20.0\n",
            "Episode 458: Reward: -20.0\n",
            "Episode 459: Reward: -21.0\n",
            "Episode 460: Reward: -21.0    Average Reward: -19.447\n",
            "Episode 461: Reward: -19.0\n",
            "Episode 462: Reward: -17.0\n",
            "Episode 463: Reward: -21.0\n",
            "Episode 464: Reward: -20.0\n",
            "Episode 465: Reward: -17.0\n",
            "Episode 466: Reward: -20.0\n",
            "Episode 467: Reward: -19.0\n",
            "Episode 468: Reward: -18.0\n",
            "Episode 469: Reward: -20.0\n",
            "Episode 470: Reward: -14.0    Average Reward: -19.355\n",
            "Episode 471: Reward: -18.0\n",
            "Episode 472: Reward: -19.0\n",
            "Episode 473: Reward: -19.0\n",
            "Episode 474: Reward: -19.0\n",
            "Episode 475: Reward: -21.0\n",
            "Episode 476: Reward: -18.0\n",
            "Episode 477: Reward: -19.0\n",
            "Episode 478: Reward: -20.0\n",
            "Episode 479: Reward: -20.0\n",
            "Episode 480: Reward: -15.0    Average Reward: -19.301\n",
            "Episode 481: Reward: -19.0\n",
            "Episode 482: Reward: -18.0\n",
            "Episode 483: Reward: -18.0\n",
            "Episode 484: Reward: -19.0\n",
            "Episode 485: Reward: -17.0\n",
            "Episode 486: Reward: -20.0\n",
            "Episode 487: Reward: -19.0\n",
            "Episode 488: Reward: -17.0\n",
            "Episode 489: Reward: -16.0\n",
            "Episode 490: Reward: -16.0    Average Reward: -19.165\n",
            "Episode 491: Reward: -21.0\n",
            "Episode 492: Reward: -21.0\n",
            "Episode 493: Reward: -18.0\n",
            "Episode 494: Reward: -19.0\n",
            "Episode 495: Reward: -20.0\n",
            "Episode 496: Reward: -21.0\n",
            "Episode 497: Reward: -19.0\n",
            "Episode 498: Reward: -21.0\n",
            "Episode 499: Reward: -20.0\n",
            "SAVED MODEL #500\n",
            "Episode 500: Reward: -18.0    Average Reward: -19.224\n",
            "Episode 501: Reward: -17.0\n",
            "Episode 502: Reward: -21.0\n",
            "Episode 503: Reward: -17.0\n",
            "Episode 504: Reward: -19.0\n",
            "Episode 505: Reward: -21.0\n",
            "Episode 506: Reward: -20.0\n",
            "Episode 507: Reward: -21.0\n",
            "Episode 508: Reward: -18.0\n",
            "Episode 509: Reward: -19.0\n",
            "Episode 510: Reward: -18.0    Average Reward: -19.213\n",
            "Episode 511: Reward: -21.0\n",
            "Episode 512: Reward: -18.0\n",
            "Episode 513: Reward: -21.0\n",
            "Episode 514: Reward: -21.0\n",
            "Episode 515: Reward: -18.0\n",
            "Episode 516: Reward: -20.0\n",
            "Episode 517: Reward: -15.0\n",
            "Episode 518: Reward: -17.0\n",
            "Episode 519: Reward: -16.0\n",
            "Episode 520: Reward: -17.0    Average Reward: -19.131\n",
            "Episode 521: Reward: -17.0\n",
            "Episode 522: Reward: -20.0\n",
            "Episode 523: Reward: -20.0\n",
            "Episode 524: Reward: -17.0\n",
            "Episode 525: Reward: -20.0\n",
            "Episode 526: Reward: -18.0\n",
            "Episode 527: Reward: -19.0\n",
            "Episode 528: Reward: -19.0\n",
            "Episode 529: Reward: -19.0\n",
            "Episode 530: Reward: -17.0    Average Reward: -19.080\n",
            "Episode 531: Reward: -20.0\n",
            "Episode 532: Reward: -18.0\n",
            "Episode 533: Reward: -19.0\n",
            "Episode 534: Reward: -20.0\n",
            "Episode 535: Reward: -18.0\n",
            "Episode 536: Reward: -18.0\n",
            "Episode 537: Reward: -19.0\n",
            "Episode 538: Reward: -18.0\n",
            "Episode 539: Reward: -18.0\n",
            "Episode 540: Reward: -20.0    Average Reward: -19.053\n",
            "Episode 541: Reward: -20.0\n",
            "Episode 542: Reward: -18.0\n",
            "Episode 543: Reward: -16.0\n",
            "Episode 544: Reward: -17.0\n",
            "Episode 545: Reward: -19.0\n",
            "Episode 546: Reward: -21.0\n",
            "Episode 547: Reward: -19.0\n",
            "Episode 548: Reward: -19.0\n",
            "Episode 549: Reward: -17.0\n",
            "SAVED MODEL #550\n",
            "Episode 550: Reward: -17.0    Average Reward: -18.980\n",
            "Episode 551: Reward: -20.0\n",
            "Episode 552: Reward: -19.0\n",
            "Episode 553: Reward: -18.0\n",
            "Episode 554: Reward: -19.0\n",
            "Episode 555: Reward: -20.0\n",
            "Episode 556: Reward: -19.0\n",
            "Episode 557: Reward: -16.0\n",
            "Episode 558: Reward: -19.0\n",
            "Episode 559: Reward: -18.0\n",
            "Episode 560: Reward: -18.0    Average Reward: -18.942\n",
            "Episode 561: Reward: -19.0\n",
            "Episode 562: Reward: -20.0\n",
            "Episode 563: Reward: -16.0\n",
            "Episode 564: Reward: -19.0\n",
            "Episode 565: Reward: -18.0\n",
            "Episode 566: Reward: -18.0\n",
            "Episode 567: Reward: -19.0\n",
            "Episode 568: Reward: -18.0\n",
            "Episode 569: Reward: -19.0\n",
            "Episode 570: Reward: -19.0    Average Reward: -18.900\n",
            "Episode 571: Reward: -18.0\n",
            "Episode 572: Reward: -20.0\n",
            "Episode 573: Reward: -18.0\n",
            "Episode 574: Reward: -19.0\n",
            "Episode 575: Reward: -20.0\n",
            "Episode 576: Reward: -18.0\n",
            "Episode 577: Reward: -18.0\n",
            "Episode 578: Reward: -19.0\n",
            "Episode 579: Reward: -15.0\n",
            "Episode 580: Reward: -20.0    Average Reward: -18.861\n",
            "Episode 581: Reward: -20.0\n",
            "Episode 582: Reward: -19.0\n",
            "Episode 583: Reward: -20.0\n",
            "Episode 584: Reward: -15.0\n",
            "Episode 585: Reward: -18.0\n",
            "Episode 586: Reward: -19.0\n",
            "Episode 587: Reward: -16.0\n",
            "Episode 588: Reward: -17.0\n",
            "Episode 589: Reward: -20.0\n",
            "Episode 590: Reward: -18.0    Average Reward: -18.797\n",
            "Episode 591: Reward: -20.0\n",
            "Episode 592: Reward: -19.0\n",
            "Episode 593: Reward: -18.0\n",
            "Episode 594: Reward: -21.0\n",
            "Episode 595: Reward: -19.0\n",
            "Episode 596: Reward: -18.0\n",
            "Episode 597: Reward: -19.0\n",
            "Episode 598: Reward: -16.0\n",
            "Episode 599: Reward: -21.0\n",
            "SAVED MODEL #600\n",
            "Episode 600: Reward: -19.0    Average Reward: -18.816\n",
            "Episode 601: Reward: -18.0\n",
            "Episode 602: Reward: -19.0\n",
            "Episode 603: Reward: -18.0\n",
            "Episode 604: Reward: -16.0\n",
            "Episode 605: Reward: -17.0\n",
            "Episode 606: Reward: -17.0\n",
            "Episode 607: Reward: -17.0\n",
            "Episode 608: Reward: -17.0\n",
            "Episode 609: Reward: -20.0\n",
            "Episode 610: Reward: -17.0    Average Reward: -18.699\n",
            "Episode 611: Reward: -18.0\n",
            "Episode 612: Reward: -20.0\n",
            "Episode 613: Reward: -20.0\n",
            "Episode 614: Reward: -19.0\n",
            "Episode 615: Reward: -19.0\n",
            "Episode 616: Reward: -18.0\n",
            "Episode 617: Reward: -20.0\n",
            "Episode 618: Reward: -20.0\n",
            "Episode 619: Reward: -20.0\n",
            "Episode 620: Reward: -17.0    Average Reward: -18.737\n",
            "Episode 621: Reward: -19.0\n",
            "Episode 622: Reward: -17.0\n",
            "Episode 623: Reward: -18.0\n",
            "Episode 624: Reward: -18.0\n",
            "Episode 625: Reward: -17.0\n",
            "Episode 626: Reward: -17.0\n",
            "Episode 627: Reward: -19.0\n",
            "Episode 628: Reward: -17.0\n",
            "Episode 629: Reward: -19.0\n",
            "Episode 630: Reward: -19.0    Average Reward: -18.667\n",
            "Episode 631: Reward: -20.0\n",
            "Episode 632: Reward: -17.0\n",
            "Episode 633: Reward: -19.0\n",
            "Episode 634: Reward: -18.0\n",
            "Episode 635: Reward: -18.0\n",
            "Episode 636: Reward: -18.0\n",
            "Episode 637: Reward: -18.0\n",
            "Episode 638: Reward: -17.0\n",
            "Episode 639: Reward: -19.0\n",
            "Episode 640: Reward: -18.0    Average Reward: -18.622\n",
            "Episode 641: Reward: -16.0\n",
            "Episode 642: Reward: -19.0\n",
            "Episode 643: Reward: -20.0\n",
            "Episode 644: Reward: -21.0\n",
            "Episode 645: Reward: -18.0\n",
            "Episode 646: Reward: -17.0\n",
            "Episode 647: Reward: -21.0\n",
            "Episode 648: Reward: -15.0\n",
            "Episode 649: Reward: -19.0\n",
            "SAVED MODEL #650\n",
            "Episode 650: Reward: -18.0    Average Reward: -18.600\n",
            "Episode 651: Reward: -17.0\n",
            "Episode 652: Reward: -20.0\n",
            "Episode 653: Reward: -19.0\n",
            "Episode 654: Reward: -19.0\n",
            "Episode 655: Reward: -18.0\n",
            "Episode 656: Reward: -18.0\n",
            "Episode 657: Reward: -17.0\n",
            "Episode 658: Reward: -19.0\n",
            "Episode 659: Reward: -18.0\n",
            "Episode 660: Reward: -17.0    Average Reward: -18.561\n",
            "Episode 661: Reward: -20.0\n",
            "Episode 662: Reward: -17.0\n",
            "Episode 663: Reward: -19.0\n",
            "Episode 664: Reward: -18.0\n",
            "Episode 665: Reward: -21.0\n",
            "Episode 666: Reward: -16.0\n",
            "Episode 667: Reward: -19.0\n",
            "Episode 668: Reward: -17.0\n",
            "Episode 669: Reward: -20.0\n",
            "Episode 670: Reward: -18.0    Average Reward: -18.555\n",
            "Episode 671: Reward: -18.0\n",
            "Episode 672: Reward: -19.0\n",
            "Episode 673: Reward: -17.0\n",
            "Episode 674: Reward: -19.0\n",
            "Episode 675: Reward: -18.0\n",
            "Episode 676: Reward: -21.0\n",
            "Episode 677: Reward: -20.0\n",
            "Episode 678: Reward: -18.0\n",
            "Episode 679: Reward: -18.0\n",
            "Episode 680: Reward: -20.0    Average Reward: -18.579\n",
            "Episode 681: Reward: -17.0\n",
            "Episode 682: Reward: -18.0\n",
            "Episode 683: Reward: -21.0\n",
            "Episode 684: Reward: -18.0\n",
            "Episode 685: Reward: -20.0\n",
            "Episode 686: Reward: -19.0\n",
            "Episode 687: Reward: -18.0\n",
            "Episode 688: Reward: -18.0\n",
            "Episode 689: Reward: -20.0\n",
            "Episode 690: Reward: -19.0    Average Reward: -18.601\n",
            "Episode 691: Reward: -18.0\n",
            "Episode 692: Reward: -17.0\n",
            "Episode 693: Reward: -15.0\n",
            "Episode 694: Reward: -16.0\n",
            "Episode 695: Reward: -16.0\n",
            "Episode 696: Reward: -20.0\n",
            "Episode 697: Reward: -18.0\n",
            "Episode 698: Reward: -15.0\n",
            "Episode 699: Reward: -18.0\n",
            "SAVED MODEL #700\n",
            "Episode 700: Reward: -19.0    Average Reward: -18.468\n",
            "Episode 701: Reward: -19.0\n",
            "Episode 702: Reward: -16.0\n",
            "Episode 703: Reward: -16.0\n",
            "Episode 704: Reward: -19.0\n",
            "Episode 705: Reward: -17.0\n",
            "Episode 706: Reward: -18.0\n",
            "Episode 707: Reward: -17.0\n",
            "Episode 708: Reward: -18.0\n",
            "Episode 709: Reward: -17.0\n",
            "Episode 710: Reward: -21.0    Average Reward: -18.406\n",
            "Episode 711: Reward: -21.0\n",
            "Episode 712: Reward: -19.0\n",
            "Episode 713: Reward: -16.0\n",
            "Episode 714: Reward: -18.0\n",
            "Episode 715: Reward: -19.0\n",
            "Episode 716: Reward: -18.0\n",
            "Episode 717: Reward: -21.0\n",
            "Episode 718: Reward: -18.0\n",
            "Episode 719: Reward: -20.0\n",
            "Episode 720: Reward: -17.0    Average Reward: -18.434\n",
            "Episode 721: Reward: -18.0\n",
            "Episode 722: Reward: -21.0\n",
            "Episode 723: Reward: -18.0\n",
            "Episode 724: Reward: -18.0\n",
            "Episode 725: Reward: -17.0\n",
            "Episode 726: Reward: -17.0\n",
            "Episode 727: Reward: -17.0\n",
            "Episode 728: Reward: -16.0\n",
            "Episode 729: Reward: -15.0\n",
            "Episode 730: Reward: -15.0    Average Reward: -18.312\n",
            "Episode 731: Reward: -18.0\n",
            "Episode 732: Reward: -17.0\n",
            "Episode 733: Reward: -20.0\n",
            "Episode 734: Reward: -20.0\n",
            "Episode 735: Reward: -16.0\n",
            "Episode 736: Reward: -17.0\n",
            "Episode 737: Reward: -16.0\n",
            "Episode 738: Reward: -19.0\n",
            "Episode 739: Reward: -18.0\n",
            "Episode 740: Reward: -13.0    Average Reward: -18.222\n",
            "Episode 741: Reward: -17.0\n",
            "Episode 742: Reward: -18.0\n",
            "Episode 743: Reward: -21.0\n",
            "Episode 744: Reward: -17.0\n",
            "Episode 745: Reward: -18.0\n",
            "Episode 746: Reward: -17.0\n",
            "Episode 747: Reward: -19.0\n",
            "Episode 748: Reward: -17.0\n",
            "Episode 749: Reward: -15.0\n",
            "SAVED MODEL #750\n",
            "Episode 750: Reward: -16.0    Average Reward: -18.151\n",
            "Episode 751: Reward: -15.0\n",
            "Episode 752: Reward: -18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iYjWVD6mez4p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "state1 = resize(env.reset())\n",
        "\n",
        "for i in range(50):\n",
        "    state2, _, _, _ = env.step(env.action_space.sample())\n",
        "    state2 = resize(state2)\n",
        "    if i > 15:\n",
        "        plt.imshow((state2-state1).reshape(80,80), cmap='gray')\n",
        "        plt.show()\n",
        "    state1 = state2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zRnaiGhDe1KC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "state1 = np.zeros([1,n_input])\n",
        "state2 = resize(env.reset())\n",
        "\n",
        "while True:\n",
        "    state = state2-state1\n",
        "    state1 = state2\n",
        "    env.render()\n",
        "    feed = {X: np.reshape(state, (1,n_input))}\n",
        "    aprob = sess.run(tf_aprob,feed) ; aprob = aprob[0,:]\n",
        "    action = np.argmax(aprob)+1\n",
        "    state2, reward, done, info = env.step(action)\n",
        "    state2 = resize(state2)\n",
        "    \n",
        "    if reward == 1:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CltFnR8He5st",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weightVal = sess.run(tf_model['W1'])\n",
        "\n",
        "for i in range(200):\n",
        "    plt.imshow(weightVal[:,i].reshape(80,80), cmap='coolwarm')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}